{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone project\n",
    "Kyle McMillan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1\n",
    "First stage is to load all the necessary modules and the dataset. The dataset is loaded and the preprocessing is carried out on the data in preperation for the classifier algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load all necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import dateutil.parser\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the dataset\n",
    "data_file = '~/Documents/Udacity/capstone/earthquakes_20170113.csv'\n",
    "df = pd.read_csv(data_file, dtype={\"publicid\": str})\n",
    "\n",
    "# Drop values less than 1800s due to datetime limitations\n",
    "df=df.drop(df.index[586892: ])\n",
    "\n",
    "#Remove event type rows with these values - not related to tectonic earthquakes\n",
    "df = df[df.eventtype != \"snow avalanche\"]\n",
    "df = df[df.eventtype != \"outside of network interest\"]\n",
    "df = df[df.eventtype != \"landslide\"]\n",
    "df = df[df.eventtype != \"sonic boom\"]\n",
    "df = df[df.eventtype != \"debris avalanche\"]\n",
    "df = df[df.eventtype != \"not locatable\"]\n",
    "df = df[df.eventtype != \"quarry blast\"]\n",
    "df = df[df.eventtype != \"explosion\"]\n",
    "df = df[df.eventtype != \"duplicate\"]\n",
    "df = df[df.eventtype != \"volcanic eruption\"]\n",
    "df = df[df.eventtype != \"nuclear explosion\"]\n",
    "df = df[df.eventtype != \"induced earthquake\"]\n",
    "df = df[df.eventtype != \"other\"]\n",
    "\n",
    "df = df.drop(df.columns[[0,1,3,8,9,10,11,12,13,14,15,16,17,18,19,20]], axis=1)\n",
    "\n",
    "#Remove these magnitudes values - data was not measured accurately\n",
    "df = df[df.magnitude >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split timestamp date out for conversion from ISO 8610 to seconds\n",
    "ISO_8610 = df['origintime']\n",
    "raw_features = df.drop('origintime', axis = 1)\n",
    "\n",
    "# Converting the ISO 8601 timestamp to seconds\n",
    "seconds = []\n",
    "for x in ISO_8610:\n",
    "    utc_dt = datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    seconds.append((utc_dt - datetime(1900, 1, 1)).total_seconds())  \n",
    "    \n",
    "raw_features['origintime_s'] = seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert the earthquake values into 0 for less than magnitude 5.0\n",
    "# and 1 for values 5.0 and above.\n",
    "quakes = []\n",
    "for x in raw_features.magnitude:\n",
    "    if x < 5:\n",
    "        quakes.append(0)\n",
    "    else:\n",
    "        quakes.append(1)\n",
    "raw_features['quakes'] = quakes\n",
    "raw_features = raw_features.drop('magnitude', axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    566934\n",
       "1      2246\n",
       "Name: quakes, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the count for each magnitude value before up-sampling\n",
    "raw_features.quakes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>origintime_s</th>\n",
       "      <th>quakes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166.787490</td>\n",
       "      <td>-46.365530</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.448276e+09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174.548230</td>\n",
       "      <td>-41.285370</td>\n",
       "      <td>30.680700</td>\n",
       "      <td>2.736774e+09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175.287182</td>\n",
       "      <td>-41.204894</td>\n",
       "      <td>21.640625</td>\n",
       "      <td>3.673823e+09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>178.009990</td>\n",
       "      <td>-36.710000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.773488e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166.770950</td>\n",
       "      <td>-45.383360</td>\n",
       "      <td>20.143100</td>\n",
       "      <td>3.270459e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176.550000</td>\n",
       "      <td>-37.230000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>2.083324e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    longitude   latitude       depth  origintime_s  quakes\n",
       "0  166.787490 -46.365530    5.000000  3.448276e+09       0\n",
       "1  174.548230 -41.285370   30.680700  2.736774e+09       0\n",
       "2  175.287182 -41.204894   21.640625  3.673823e+09       0\n",
       "0  178.009990 -36.710000   12.000000  1.773488e+09       1\n",
       "1  166.770950 -45.383360   20.143100  3.270459e+09       1\n",
       "2  176.550000 -37.230000  348.000000  2.083324e+09       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select six random events from the dataset\n",
    "# Three values are events with a 5.0 or greater earthquake and 3 are less than 5.0\n",
    "samples_1 = pd.DataFrame(raw_features[raw_features.quakes == 1].sample(n=3, random_state=42), \n",
    "                         columns = raw_features.keys()).reset_index(drop = True)\n",
    "samples_0 = pd.DataFrame(raw_features[raw_features.quakes == 0].sample(n=3, random_state=42), \n",
    "                         columns = raw_features.keys()).reset_index(drop = True)\n",
    "\n",
    "total_samples = pd.concat([samples_0, samples_1])\n",
    "\n",
    "display(total_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2\n",
    "The second stage is to calculate the Naive predictor using the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive predictor: [Accuracy score: 0.003946027618679504, F-score: 0.010049732457494956]\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy, precision and recall\n",
    "accuracy = np.sum(quakes) / float(len(quakes))\n",
    "recall = np.sum(quakes) / float(np.sum(quakes) + 0)\n",
    "precision = np.sum(quakes) / float(np.sum(quakes) + quakes.count(0))\n",
    "\n",
    "# Calculate F-score using beta = 1.25.\n",
    "beta = 1.25\n",
    "fscore = ((1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall))\n",
    "\n",
    "# Print the results \n",
    "print (\"Naive predictor: [Accuracy score: {}, F-score: {}]\".format(accuracy, fscore))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3\n",
    "The third stage is to up-sample the data. Because the original dataset is heavily imbalanced,\n",
    "this actions ensures that when the training and testing sets are split, all of the minority data doesn't end up \n",
    "in either only training or only testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Up-sampling of the earthquake data so that dataset \n",
    "# is no long imbalanced.\n",
    "\n",
    "quakes_majority = raw_features[raw_features.quakes==0]\n",
    "quakes_minority = raw_features[raw_features.quakes==1]\n",
    "\n",
    "quakes_minority_upsampled = resample(quakes_minority, replace=True, n_samples=566934, random_state=42)\n",
    "quakes_upsampled = pd.concat([quakes_majority, quakes_minority_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    566934\n",
       "0    566934\n",
       "Name: quakes, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the count for each magnitude value after up-sampling\n",
    "quakes_upsampled.quakes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4\n",
    "The data is split into features and lables along with testing and training sets for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split quake data into features and target label\n",
    "\n",
    "quake_lables = quakes_upsampled['quakes']\n",
    "quake_features = quakes_upsampled.drop('quakes', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has 793707 samples.\n",
      "The testing set has 340161 samples.\n"
     ]
    }
   ],
   "source": [
    "#split the up-sampled dataset into training(70%) and testing(30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(quake_features, quake_lables, test_size = 0.3, random_state = 42)\n",
    "\n",
    "\n",
    "# Show the results of the split\n",
    "print (\"The training set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"The testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 5\n",
    "The Random Forest Classifier is trained and tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_samples_leaf = 6.8\n",
      "{'f_train': 0.99988596467190438, 'f_test': 0.99926601422212125}\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, \n",
    "                             oob_score=True, \n",
    "                             min_samples_leaf=2, \n",
    "                             max_features=\"sqrt\", \n",
    "                             max_depth=50, \n",
    "                             random_state = 42)\n",
    "results = {}\n",
    "    \n",
    "clf = clf.fit(X_train, y_train)\n",
    "        \n",
    "#Get the predictions on the test set then get predictions on the training samples\n",
    "predictions_test = clf.predict(X_test)\n",
    "predictions_train = clf.predict(X_train)\n",
    "          \n",
    "#Compute F-score on the the training samples using f-beta\n",
    "results['f_train'] = fbeta_score(y_train, predictions_train, 1.25)\n",
    "#Compute F-score on the test set using f-beta\n",
    "results['f_test'] = fbeta_score(y_test, predictions_test,1.25)\n",
    "       \n",
    "\n",
    "print (\"min_samples_leaf = {}\".format(x))    \n",
    "print (results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Stage 6\n",
    "This final section will test the classifier.\n",
    "Six sample points removed above will be inputted into the classifer to dertermine if the classifer can correctly identify the sample\n",
    "\n",
    "<br>Additionly, a single new point was recorded as a magnitude 5.06, but was recorded approximatly two weeks after the original dataset was downloaded, will be use to test if the classifer can correctly future points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 1]\n",
      "[[  1.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+00   0.00000000e+00]\n",
      " [  8.62419456e-04   9.99137581e-01]\n",
      " [  7.28752921e-04   9.99271247e-01]\n",
      " [  3.83979189e-04   9.99616021e-01]]\n"
     ]
    }
   ],
   "source": [
    "#Prediciton of the sample sets\n",
    "\n",
    "#'quakes' column is dropped as that is the answer trying to predict\n",
    "print(clf.predict(total_samples.drop('quakes', axis= 1)))\n",
    "print(clf.predict_proba(total_samples.drop('quakes', axis= 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    longitude   latitude       depth  origintime_s  quakes\n",
      "0  173.974579 -40.340115  123.553619  3.726009e+09       1\n"
     ]
    }
   ],
   "source": [
    "#Import the dataset with a future dated earthquake \n",
    "data_file = '~/Documents/Udacity/capstone/test_earthquake.csv'\n",
    "df_test = pd.read_csv(data_file, dtype={\"publicid\": str})\n",
    "\n",
    "df_test = df_test.drop(df_test.columns[[0,1,3,6,8,9,10,11,12,13,14,15,16,17,18,19,20]], axis=1)\n",
    "\n",
    "ISO_8610 = df_test['origintime']\n",
    "test_features = df_test.drop('origintime', axis = 1)\n",
    "seconds = []\n",
    "for x in ISO_8610:\n",
    "    utc_dt = datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    seconds.append((utc_dt - datetime(1900, 1, 1)).total_seconds())\n",
    "test_features['origintime_s'] = seconds\n",
    "\n",
    "test_features['quakes'] = 1\n",
    "\n",
    "print(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#Prediciton of the future point\n",
    "\n",
    "print(clf.predict(test_features.drop('quakes', axis= 1)))\n",
    "print(clf.predict_proba(test_features.drop('quakes', axis= 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
